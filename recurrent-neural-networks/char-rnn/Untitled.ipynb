{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100,  13,  13, ...,  59,  57,  13])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file = 'data/anna.txt'\n",
    "file = 'data/kieu.txt'\n",
    "with open(file, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "vocab = list(set(text))\n",
    "id2char = dict(enumerate(vocab))\n",
    "char2id = {v: k for k, v in id2char.items()}\n",
    "txt = np.array(list(char2id[c] for c in text))\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoc 0, trn_loss = 45.32880783081055, val_loss = 45.557899475097656\n",
      "epoc 1, trn_loss = 44.951168060302734, val_loss = 45.50582504272461\n",
      "epoc 2, trn_loss = 44.94255447387695, val_loss = 45.48057556152344\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encoding(arr, vocab_len):\n",
    "    \"\"\"\n",
    "    Return a 3D tensor for each of training example. The first dimension is going to be batch\n",
    "    \"\"\"\n",
    "    inp = np.asarray(arr)\n",
    "    result = np.zeros((np.multiply(*inp.shape), vocab_len), dtype=np.float32)\n",
    "    result[np.arange(result.shape[0]), inp.flatten()] = 1\n",
    "    return result.reshape((*inp.shape, vocab_len))\n",
    "# one_hot_encoding([[2, 3, 5]], 8)\n",
    "\n",
    "def get_batch_data(data, bs, seqlen):\n",
    "    \"\"\"\n",
    "    keep the right amout of data to be fed into the NN,\n",
    "    return x, y of \n",
    "    \"\"\"\n",
    "    bn = len(data)//(bs*seqlen)\n",
    "    dat = np.array(list(data[:(bn*bs*seqlen)]))\n",
    "    dat = dat.reshape(bs, -1)\n",
    "    for i in range((bn-1)*seqlen-1):\n",
    "        x = dat[:, i: i+seqlen]\n",
    "        y = dat[:, i+1: i+seqlen+1]\n",
    "        yield x, y\n",
    "\n",
    "# for x, y in get_batch_data(txt, 64, 100):\n",
    "#     print(x.shape, y.shape)\n",
    "\n",
    "seqlen = 100\n",
    "bs = 16\n",
    "n_hidden = 512\n",
    "n_layers = 10\n",
    "clip = 5\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_hidden, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(len(vocab), n_hidden, n_layers, dropout=.2, batch_first=True)\n",
    "        self.fc = nn.Linear(n_hidden, len(vocab))\n",
    "        self.dropout = nn.Dropout(.2)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.dropout(x)\n",
    "        x = x.contiguous().view(-1, self.n_hidden)\n",
    "        x = self.fc(x)\n",
    "        return x, hidden\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        return (torch.zeros((self.n_layers, bs, self.n_hidden)).cuda(),\n",
    "                torch.zeros(self.n_layers, bs, self.n_hidden).cuda())\n",
    "    \n",
    "net = Net(n_hidden, n_layers)\n",
    "net.cuda()\n",
    "n_epocs = 3\n",
    "val_idx = int(len(txt)*.05)\n",
    "val, trn = txt[:val_idx], txt[:val_idx]\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "counter = 0\n",
    "for e in range(n_epocs):\n",
    "    net.train()\n",
    "    total_trn_loss = 0\n",
    "    hidden = net.init_hidden(bs)\n",
    "    for x, y in get_batch_data(trn, bs=bs, seqlen=seqlen):\n",
    "        counter += 1\n",
    "        x = torch.from_numpy(one_hot_encoding(x, len(vocab))).cuda()\n",
    "        y = torch.from_numpy(y).cuda().reshape(bs*seqlen)\n",
    "        out, hidden = net.forward(x, hidden)\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        loss = crit(out, y)\n",
    "        total_trn_loss += loss.data/bs\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        # to prevent exploding gradient\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "    net.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        val_h = net.init_hidden(bs)\n",
    "        for x, y in get_batch_data(val, bs=bs, seqlen=seqlen):\n",
    "            x = torch.from_numpy(one_hot_encoding(x, len(vocab))).cuda()\n",
    "            y = torch.from_numpy(y).cuda().view(bs*seqlen)\n",
    "            val_h = tuple([each.data for each in val_h])\n",
    "            out, val_h = net.forward(x, val_h)\n",
    "            val_loss = crit(out, y)\n",
    "            total_val_loss += val_loss.data/bs\n",
    "    net.train()\n",
    "    print(f\"epoc {e}, trn_loss = {total_trn_loss}, val_loss = {total_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
